output_dir: experiments/${project}/${name}/${now:%Y-%m-%d}-${now:%H-%M-%S}

project: pretrain_mask-based-wpd_LibriMixDemandDual2sec7ch
name: mask-based-wpd_doa-aware-lstm

debug: false

testonly: false
# ckpt_path: /n/work1/fujita/research/WPD_adaptation/experiments/pretrain_mask-based-wpd_LibriMixDemandDual2sec7ch/mask-based-wpd_doa-aware-lstm/2024-07-26-21-21-53/epoch=16-step=153000.ckpt

n_gpus: 6

data_module:
  _target_: src.dataset.doa_aware_dataset.LightningDataModule

  batch_size: 24
  num_workers: 7
  n_gpus: ${n_gpus}

  tr_path: /n/work3/fujita/manifest/libri_mix_demand_dual_2sec_100h_7ch_train.manifest
  vl_path: /n/work3/fujita/manifest/libri_mix_demand_dual_2sec_10h_7ch_dev.manifest
  tt_path: /n/work3/fujita/manifest/libri_mix_demand_dual_2sec_10h_7ch_test.manifest

  tr_size: -1
  vl_size: -1
  tt_size: -1

lightning_module:
  _target_: src.modules.mask_based_wpd.MaskBasedWPD

  lr: 1e-4
  skip_nan_grad: True

  delay: 3
  tap: 5

  mask_estimator:
    _target_: src.models.doa_aware_lstm.DOAAwareLSTM
    n_mics: 7
    n_freq: 513 
    input_lstm: 1024
    hidden_lstm: 512
    n_layer: 3

trainer:
  devices: ${n_gpus}
  max_epochs: 20
  max_steps: -1
  val_check_interval: 1.0
  accelerator: auto
  strategy: ddp
  accumulate_grad_batches: 4.0
  gradient_clip_val: 1.0
  precision: 64
  log_every_n_steps: 30
  enable_progress_bar: True
  num_sanity_val_steps: 0
  check_val_every_n_epoch: 1
  sync_batchnorm: True
  benchmark: True
  default_root_dir: ${output_dir}
  use_distributed_sampler: False

callbacks:
- _target_: pytorch_lightning.callbacks.ModelCheckpoint
  dirpath: ${output_dir}
  monitor: valid_loss
  save_top_k: 3
  save_last: False

logger: 
  _target_: pytorch_lightning.loggers.WandbLogger
  project: ${project}
  name: ${name}
  save_dir: ${output_dir}

hydra: 
  run: 
    dir: ${output_dir}
